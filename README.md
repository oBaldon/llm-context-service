# ğŸ§  LLM Context Service

Este Ã© um microserviÃ§o para **anÃ¡lise de contexto e significado de textos transcritos**, utilizando modelos LLM da famÃ­lia **LLaMA (Meta)**.  
Ele recebe via API REST um texto e retorna uma resposta estruturada com **contexto, tÃ³picos principais e resumo**, baseado no modelo selecionado.

---

## ğŸš€ Funcionalidades

- Recebe texto bruto (como transcriÃ§Ã£o de Ã¡udio) via endpoint HTTP.
- Analisa contexto e significado com LLMs.
- Suporte a mÃºltiplos modelos:
  - `meta-llama/Llama-3.2-1B`
  - `meta-llama/Llama-3.2-1B-Instruct`
  - `meta-llama/Llama-3.2-3B`
  - `meta-llama/Llama-3.2-3B-Instruct`
- Estrutura de resposta padronizada no formato:
  - `"system"`, `"user"` e `"assistant"`
- Salvamento automÃ¡tico do resultado como `.json` e `.txt`.

---

## âœ… Requisitos

- **Python 3.12+**
- **Token HuggingFace** (necessÃ¡rio para modelos *Instruct*)
- DependÃªncias definidas em `requirements.txt`

---

## âš™ï¸ InstalaÃ§Ã£o e ExecuÃ§Ã£o

### 1. Clonar o repositÃ³rio
```bash
git clone https://github.com/seu_usuario/llm-context-service.git
cd llm-context-service
```

### 2. Criar e ativar o ambiente virtual
```bash
python3 -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
```

### 3. Instalar as dependÃªncias
```bash
pip install -r requirements.txt
```

### 4. ConfiguraÃ§Ã£o do `.env`

> â„¹ï¸ **Importante:** Certifique-se de criar um arquivo `.env` na raiz do projeto antes de iniciar o microserviÃ§o.  
> Esse arquivo deve conter a variÃ¡vel de ambiente necessÃ¡ria para autenticar no HuggingFace.

```env
HF_TOKEN=seu_token_huggingface_aqui
```

> â— Modelos *Instruct* exigem login/autenticaÃ§Ã£o. Mesmo com o token no `.env`, pode ser necessÃ¡rio executar:
```bash
huggingface-cli login
```

---

### 5. Iniciar o microserviÃ§o
```bash
uvicorn app:app --host 0.0.0.0 --port 8000
```

---

## ğŸ“¡ Uso da API

### **Endpoint:** `/analyze`
- **MÃ©todo:** `POST`
- **DescriÃ§Ã£o:** Recebe um texto e retorna o contexto, significado e resumo do conteÃºdo.
- **ParÃ¢metros do corpo da requisiÃ§Ã£o:**
  - `text`: string com o conteÃºdo a ser analisado
  - `model`: string com o nome do modelo (ex: `llama-3.2-1b-instruct`)

### **Exemplo de requisiÃ§Ã£o com `curl`:**
```bash
curl -X POST http://localhost:8000/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "text": "'"$(cat ./tests/sample_text.txt | sed 's/"/\"/g')"'", 
    "model": "llama-3.2-1b-instruct"
  }'
```

### **Exemplo de resposta JSON:**
```json
{
  "result": [
    {
      "role": "system",
      "content": "VocÃª Ã© um assistente Ãºtil e especialista em linguagem natural."
    },
    {
      "role": "user",
      "content": "Texto completo enviado no prompt"
    },
    {
      "role": "assistant",
      "content": "Resposta do modelo com resumo e tÃ³picos estruturados"
    }
  ]
}
```

---

## ğŸ“¦ ExecuÃ§Ã£o com Docker

### 1. Construir a imagem Docker
```bash
docker build -t llm-context-service .
```

### 2. Executar o contÃªiner
```bash
docker run -p 8000:8000 llm-context-service
```

### 3. Ou executar com Docker Compose
```bash
docker-compose up --build
```

---

## ğŸ“ Estrutura do Projeto

```
llm-context-service/
â”‚
â”œâ”€â”€ app.py                  # AplicaÃ§Ã£o principal FastAPI
â”œâ”€â”€ config/                 # ConfiguraÃ§Ã£o e variÃ¡veis de ambiente
â”œâ”€â”€ models/                 # Diferentes implementaÃ§Ãµes de modelos LLaMA
â”œâ”€â”€ services/               # OrquestraÃ§Ã£o dos modelos
â”œâ”€â”€ utils/                  # Salvamento do resultado em arquivos
â”œâ”€â”€ tests/                 # Textos de entrada para testes da API
â”œâ”€â”€ temp/                   # SaÃ­da automÃ¡tica gerada (.json/.txt)
â”œâ”€â”€ Dockerfile              # DefiniÃ§Ã£o da imagem Docker
â”œâ”€â”€ docker-compose.yml      # OrquestraÃ§Ã£o com Docker Compose
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â””â”€â”€ README.md               # DocumentaÃ§Ã£o do projeto
```

---

## ğŸ›  Tecnologias Utilizadas

- **Python 3.12**
- **FastAPI**
- **Hugging Face Transformers**
- **Modelos Meta-LLaMA**
- **Docker & Docker Compose**

---

## ğŸ‘¤ Autor

Desenvolvido por [@oBaldon](https://github.com/oBaldon)

ContribuiÃ§Ãµes e melhorias sÃ£o bem-vindas!

---

### ğŸ“ Roadmap TÃ©cnico

Para conhecer os prÃ³ximos passos planejados para a evoluÃ§Ã£o do `llm-context-service`, melhorias tÃ©cnicas, testes e escalabilidade, consulte o [ğŸ“ Roadmap TÃ©cnico](./ROADMAP.md).  
Esse documento detalha as fases de desenvolvimento e prioridades recomendadas para o projeto.